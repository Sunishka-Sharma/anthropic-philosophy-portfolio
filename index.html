<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Philosophy Papers Portfolio | Sunishka Sharma</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Playfair+Display:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <header>
        <nav>
            <div class="logo">
                <img src="1730877182660.jpeg" alt="Sunishka Sharma">
            </div>
            <ul>
                <li><a href="index.html" class="active">Home</a></li>
                <li><a href="core-portfolio.html">Core Portfolio</a></li>
                <li><a href="additional-research.html">Additional Research</a></li>
                <li><a href="supplemental-work.html">Supplemental Work</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section class="hero">
            <div class="container">
                <div class="hero-profile">
                    <h1>Philosophy Papers for Anthropic</h1>
                    <p class="subtitle">Academic Portfolio in Philosophy, AI Ethics, and Cognitive Systems (2020–2024)</p>
                </div>
                <div class="hero-description">
                    <p>This collection represents my research across philosophy, AI ethics, behavioral evaluation, and technological impact. My work directly addresses Anthropic's core concerns: building models that demonstrate good judgment, align with human values, and behave responsibly across contexts.</p>
                </div>
            </div>
        </section>

        <section class="overview">
            <div class="container">
                <h2><i class="section-icon fas fa-search"></i> Overview</h2>
                
                <p>I approach alignment challenges with both normative clarity and empirical grounding. At Adobe, I currently work in the <strong>Illustrator team </strong> building <strong>Illustrator, Firefly</strong> and <strong>Adobe Concept</strong> within the <strong>GenAI division</strong>, developing creative multi-modal AI systems that require ethically aligned and user-sensitive outputs. This professional experience directly informs my ability to assess model behavior under real-world constraints and edge-case use scenarios along with fine tuning them!</p>
                
            </div>
        </section>

        <section class="focus-areas">
            <div class="container">
                <h2><i class="section-icon fas fa-book"></i> Core Focus Areas</h2>
                <div class="card-grid">
                    <div class="card">
                        <h3>Algorithmic Accountability</h3>
                        <p>Frameworks for oversight, justice, and institutional transparency in automated systems</p>
                    </div>
                    <div class="card">
                        <h3>Value Alignment</h3>
                        <p>Capturing normative human values in diverse cultural contexts</p>
                    </div>
                    <div class="card">
                        <h3>Ethical Implications</h3>
                        <p>Navigating conflicts between deontological and consequentialist reasoning</p>
                    </div>
                    <div class="card">
                        <h3>Human-AI Interaction</h3>
                        <p>Behavioral cues, trust, and user impact from AI outputs</p>
                    </div>
                    <div class="card">
                        <h3>Cognitive Science Applications</h3>
                        <p>Understanding neural behavior to improve model alignment with cognitive expectations</p>
                    </div>
                    <div class="card">
                        <h3>Animal Ethics in AI</h3>
                        <p>Exploring how non-human welfare is overlooked in AI design, and advocating for expanding the alignment discourse beyond humans</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="research">
            <div class="container">
                <h2><i class="section-icon fas fa-flask"></i> Research Experience</h2>
                <div class="research-item">
                    <h3><i class="fas fa-brain"></i> Cognitive Science Lab (2021–2022)</h3>
                    <p><strong>Project</strong>: Deep Learning for Neuropsychiatry and ADHD Diagnosis</p>
                    <p>I developed machine learning models to detect cognitive patterns for ADHD diagnosis, working with EEG and behavioral data.</p>
                    <div class="takeaways">
                        <h4>Key Takeaways for Alignment:</h4>
                        <ul>
                            <li>Behavioral pattern recognition</li>
                            <li>Context-sensitive evaluation design</li>
                            <li>Empirical grounding in cognitive variability</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <section class="approaches">
            <div class="container">
                <h2><i class="section-icon fas fa-compass"></i> Philosophical Approaches</h2>
                <ul class="approach-list">
                    <li><strong>Disclosive Ethics</strong>: Uncovering embedded values in algorithms and LLMs</li>
                    <li><strong>Critical Theory</strong>: Power dynamics in AI infrastructure</li>
                    <li><strong>Post-Phenomenology</strong>: Human-tech relations and experience design</li>
                    <li><strong>Virtue Ethics</strong>: Judgment, character, and moral development in intelligent systems</li>
                    <li><strong>Moral Psychology</strong>: Human ethical reasoning as a model for alignment</li>
                </ul>
            </div>
        </section>

        <section class="portfolio">
            <div class="container">
                <h2><i class="section-icon fas fa-thumbtack"></i> Core Portfolio</h2>
                <p>Five papers most relevant to alignment, model behavior, and algorithmic ethics:</p>
                <ol class="paper-list">
                    <li><strong>Algorithmic Accountability and Public Reason</strong></li>
                    <li><strong>Family Vlogging and YouTube's Algorithm</strong></li>
                    <li><strong>Beyond Humans: The Case for Animals in AI Ethics</strong></li>
                    <li><strong>The Ethics of Family Vlogging</strong></li>
                    <li><strong>Autonomy and Free Will in the Digital Age</strong></li>
                </ol>
                <a href="core-portfolio.html" class="button">View Core Portfolio</a>
            </div>
        </section>

        <section class="additional">
            <div class="container">
                <h2><i class="section-icon fas fa-lightbulb"></i> Additional Research</h2>
                <p>Wider ethical framing of AI systems and sociotechnical design:</p>
                <ol class="paper-list">
                    <li><strong>Why AI Is Not Neutral</strong></li>
                    <li><strong>The Limits of Utilitarianism in Public Policy</strong></li>
                    <li><strong>Drones and the Ethics of Remote Warfare</strong></li>
                </ol>
                <a href="additional-research.html" class="button">View Additional Research</a>
            </div>
        </section>

        <section class="supplemental">
            <div class="container">
                <h2><i class="section-icon fas fa-paperclip"></i> Supplemental Work</h2>
                <p>Theoretical background and philosophical range:</p>
                <ol class="paper-list">
                    <li><strong>Ethical Challenges in the COVID-19 Vaccine Rollout</strong> <em>(Primary Author)</em></li>
                    <li><strong>Demystifying Actor-Network Theory (ANT)</strong></li>
                    <li><strong>Durkheim and the Division of Labour</strong></li>
                </ol>
                <a href="supplemental-work.html" class="button">View Supplemental Work</a>
            </div>
        </section>

        <section class="academic">
            <div class="container">
                <h2><i class="section-icon fas fa-graduation-cap"></i> Academic Background</h2>
                
                <h3><i class="fas fa-book-open"></i> Philosophy & Psychology Training</h3>
                <p>I enrolled in <strong>every philosophy elective</strong> offered at IIIT-Delhi and received a <strong>perfect GPA (10/10)</strong> in all of them. These were taught by active researchers in ethics, political theory, and technology studies. Courses were attended alongside <strong>Master's and PhD students</strong> from philosophy and social science backgrounds. In most classes, <strong>only two students scored above a GPA of 9</strong>, due to the relative grading policy.</p>
                
                <h3><i class="fas fa-file-alt"></i> Philosophy Courses Completed with Distinction</h3>
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Course Title</th>
                                <th>Instructor</th>
                                <th>Semester</th>
                                <th>Grade</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Ethics in AI</strong></td>
                                <td>Dr. Manohar Kumar</td>
                                <td>Spring 2023</td>
                                <td>A (10)</td>
                            </tr>
                            <tr>
                                <td><strong>Philosophy of Technology</strong></td>
                                <td>Dr. Nishad Patnaik</td>
                                <td>Fall 2022</td>
                                <td>A (10)</td>
                            </tr>
                            <tr>
                                <td><strong>Social and Political Philosophy</strong></td>
                                <td>Dr. Manohar Kumar</td>
                                <td>Fall 2021</td>
                                <td>A (10)</td>
                            </tr>
                            <tr>
                                <td><strong>Theory & Practice of Engineering Ethics</strong></td>
                                <td>Dr. Shweta Singh</td>
                                <td>Spring 2021</td>
                                <td>A (10)</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <p>These foundational courses shaped my ethical perspective on AI design, behavior evaluation, and long-term alignment challenges.</p>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-info">
                    <h3>Sunishka Sharma</h3>
                    <p>Philosophy & AI Ethics Portfolio</p>
                    <p>&copy; 2025 All Rights Reserved</p>
                </div>
                <div class="footer-nav">
                    <h4>Navigation</h4>
                    <div class="footer-links">
                        <a href="index.html">Home</a> | 
                        <a href="core-portfolio.html">Core Portfolio</a> | 
                        <a href="additional-research.html">Additional Research</a> | 
                        <a href="supplemental-work.html">Supplemental Work</a>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>
</html> 